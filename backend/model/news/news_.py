# -*- coding: utf-8 -*-
"""News_fix.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I9myKzEVUKZ-TuNvg02F21KRnOWwaXd4
"""

!pip install pororo --use-deprecated=legacy-resolver
!pip install python-mecab-ko
!pip install keybert
!pip install hanja

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# apt-get update
# apt-get install g++ openjdk-8-jdk python-dev python3-dev
# pip3 install JPype1
# pip3 install konlpy
# pip3 install gensim
# bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)
# pip3 install /tmp/mecab-python-0.996

# Commented out IPython magic to ensure Python compatibility.
# %env JAVA_HOME "/usr/lib/jvm/java-8-openjdk-amd64"

# Commented out IPython magic to ensure Python compatibility.
!bash /tmp/mecab-ko-dic-2.1.1-20180720/tools/add-userdic.sh
# %cd ../tmp/mecab-ko-dic-2.1.1-20180720
!make install

# Pororo, Hannanum, Mecab Test
from konlpy.tag import Hannanum
from konlpy.tag import Mecab
from pororo import Pororo

hannanum = Hannanum()
hannanum.morphs("한나눔을 테스트 합니다.")
mecab = Mecab()
mecab.morphs("메캅을 테스트 합니다.")
dp = Pororo(task="dep_parse", lang="ko")
dp("안녕하세요.")

from konlpy.tag import Hannanum
from konlpy.tag import Mecab
from bs4 import BeautifulSoup
from konlpy.utils import pprint
import requests
import time
import hanja
import re
from datetime import datetime, timedelta

def clean_text(new):
    new = new.replace('美', '미국')
    new = new.replace('韓', '한국')
    new = new.replace('中', '중국')
    new = new.replace('日', '일본')
    new = new.replace('英', '영국')
    new = new.replace('獨', '독일')
    new = new.replace('强', '강세')
    new = new.replace('↑', ' 상승')
    new = new.replace('↓', ' 하락')
    new = new.replace('-', '감소')
    new = new.replace('&amp;', '&')
    new = re.sub('\[.*\]', '', new)
    new = re.sub("[-=+,#/\?:^@*\"※~ㆍ!』‘|\(\)\[\]`\'…》\”\“\’·◆]"," ",new)
    new = ' '.join(new.split())
    new = hanja.translate(new, 'substitution')
    return new

def crawling_news(start, last):

  start_date = datetime.strptime(start, '%Y-%m-%d')
  last_date = datetime.strptime(last, '%Y-%m-%d')

  data = {}
  split_data = {}
  new_string_split = []

  while start_date <= last_date:
      dates = start_date.strftime('%Y-%m-%d')
      print(dates)
      url = f"https://finance.naver.com/news/mainnews.naver"
      req = requests.get(url, headers={'User-agent': 'Mozilla/5.0'}, params={'date': dates})
      html = BeautifulSoup(req.text, 'lxml')
      news = html.find_all('dd', class_="articleSubject")
      news_title=[]
      news_title_split=[]
      for new in news:
          new = new.find('a')
          news_href = "https://finance.naver.com" + new.attrs['href']
          new_string = ""

          # subtracting news summary or first paragraph
          req = requests.get(news_href, headers={'User-agent': 'Mozilla/5.0'})
          html = BeautifulSoup(req.text, 'lxml')
          summary = html.find(class_="media_end_summary")

          if summary:
            summary = re.sub('<[^>]+>'," ",str(summary))
            new_string = summary
            new_string_split = summary.split('.')
          else:
            article = html.find(class_="articleCont")
            if article is not None and article.find('span') is not None:
              article.find('span').decompose()
              article = re.sub('<div class="articleCont" id="content">',"",str(article))
              article = re.search('<[^>]+>[^<]+<[^>]+>', str(article))
              article = re.sub('<[^>]+>',"",str(article.group()))
              article = re.sub('<[^>]+>'," ",str(article))
              new_string = article
              new_string_split = str(article).split('.')
          

          # subtracting news title
          new_string += new.string
          new_string_split.append(new.string)
          new_string = clean_text(new_string)
          for sent in new_string_split:
            news_title_split.append(clean_text(sent))
          news_title.append(new_string)
    
      data[dates] = news_title
      split_data[dates] = news_title_split
    
      time.sleep(0.01)
      start_date += timedelta(days=1)

  print(data)
  return data, split_data

data, split_data = crawling_news('2022-11-01', '2022-11-30')

from keybert import KeyBERT
from sentence_transformers import SentenceTransformer

sentence_model = SentenceTransformer('jhgan/ko-sroberta-multitask')
kw_model = KeyBERT(model=sentence_model)

# Intialize stopword, keyword_pair
stopword =[] $# initialize stopwords
keyword_pair = {} # storing keyword_pair by dictionary
ncpwords = ['급락'] # initalize custom ncp_words(hannanum에 없는 상태성, 동작성 명사를 개인적으로 추가합니다.)

#해당 날짜의 뉴스데이터를 모아 Keyword 추출, Keyword 여러개의 경우 + Dependency Parser


start = '2022-11-01'
last = '2022-11-30'


def news_extract(start, last):

  start_date = datetime.strptime(start, '%Y-%m-%d')
  last_date = datetime.strptime(last, '%Y-%m-%d')

  while start_date <= last_date:
      dates = start_date.strftime('%Y-%m-%d')
      
      if dates in keyword_pair:
        start_date += timedelta(days=1)
        continue

      # 키워드 추출 후 TF 구하기
      news_list = data[dates]
      keywords_count = {}
      for news in news_list:
          news = " ".join(mecab.morphs(news))
          keywords = kw_model.extract_keywords(news, top_n=10, stop_words=["이미지", "본문이미지"])

          for keyword in keywords:
            hannanum_list = hannanum.analyze(keyword[0])
            NCP = False

            for temp1 in hannanum_list:
              for temp2 in temp1:
                for analyze_word in temp2:
                  if analyze_word[1] == 'ncpa' or analyze_word[1] == 'ncps' or keyword[0] in ncpwords:
                    NCP = True           
            if NCP == False :
              if keyword[0] in keywords_count:
                keywords_count[keyword[0]][0] += 1
                if keywords_count[keyword[0]][1] < keyword[1] : 
                  keywords_count[keyword[0]][1] = keyword[1]
              else:
                keywords_count[keyword[0]] = [1, keyword[1]]
      
      keywords_count = sorted(keywords_count.items(), key = lambda item:( (-item[1][0]), (-item[1][1])))
      i = 0
      if(len(keywords_count)==0):
        keyword_pair[dates] = ["", ""]
        start_date += timedelta(days=1)
        continue

      top_keyword = []
      n = 3 # 일별 뽑을 개수를 설정해줍니다.
      while len(top_keyword) < n and i<len(keywords_count):
        if keywords_count[i][0].isdigit() != True :
          top_keyword +=[keywords_count[i][0]]
        i = i + 1
      print('keyword_list', top_keyword)
      
      top_keyword_ok = {}
      for keyword in top_keyword:
        top_keyword_ok[keyword] = False

      # Dependency Parser를 이용해 연관된 상태성, 동작성 명사 추출
      keyword2 = ""
      work_word = ""
      news_list = data[dates]
      result = ""
      date_keyword = []

      for news in news_list:
        news_split = news.split('.')
        for new in news_split:
          for keyword in top_keyword:
            if keyword in new: 
              new = ' '.join(mecab.morphs(new))   

              try:
                dp_news = dp(new)
              except:
                continue

              for word in dp_news:
                if keyword == word[1]:
                  top_keyword_index = word[0]
                  next_index = word[2] - 1
                  while True :
                    word = dp_news[next_index]
                    if word[1]=='하' and word[3]=='VP':
                      word = dp_news[next_index-1]
                    hannanum_list = hannanum.analyze(word[1])

                    END = False
                    
                    # find related verb using hannanum & dependency parser
                    for temp2 in hannanum_list[0]:
                      for analyze_word in temp2:
                        if analyze_word[1] == 'ncpa' or analyze_word[1] == 'ncps':
                          work_word = word[1]
                          END = True
                          break
                      if END:
                        break

                    if END:
                      for word_temp in dp_news:
                        if word_temp[2] == top_keyword_index and word_temp[3]=='NP':
                          keyword_concat = word_temp[1] + keyword
                      if not top_keyword_ok[keyword]:
                        date_keyword.append([(keyword_concat if keyword_concat else keyword), work_word, news])
                        top_keyword_ok[keyword] = True

                    if END == True or dp_news[next_index][0] == len(dp_news):
                      keyword2 = word[1]
                      break
                    curr_index = next_index
                    next_index = dp_news[next_index][2] - 1
                    if next_index == -2 or next_index < curr_index :
                      keyword2 = word[1]
                      break

            if result != "":
              top_keyword = result
            if work_word != "":
              keyword2 = work_word

      keyword_pair[dates] = date_keyword
      print(dates, date_keyword)
      start_date += timedelta(days=1)

news_extract(start, last)

print(keyword_pair)

# keyword_pair를 json file로 export 합니다.

import json

with open("news_keyword.json", 'w') as outfile:
    json.dump(keyword_pair, outfile, ensure_ascii=False)